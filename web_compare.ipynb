{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a50819a5",
   "metadata": {},
   "source": [
    "# Web Mining and Applied NLP (CSIS 44-620)\n",
    "\n",
    "## P7: Custom Final Project: Web Mining & NLP\n",
    "\n",
    "### \n",
    "Author: Data-Git-Hub <br>\n",
    "GitHub Project Repository Link: https://github.com/Data-Git-Hub/p7_web_compare <br>\n",
    "26 July 2025 <br>\n",
    "\n",
    "### Introduction\n",
    "Need <br>\n",
    "\n",
    "### Imports\n",
    "Python libraries are collections of pre-written code that provide specific functionalities, making programming more efficient and reducing the need to write code from scratch. These libraries cover a wide range of applications, including data analysis, machine learning, web development, and automation. Some libraries, such as os, sys, math, json, and datetime, come built-in with Python as part of its standard library, providing essential functions for file handling, system operations, mathematical computations, and data serialization. Other popular third-party libraries, like `pandas`, `numpy`, `matplotlib`, `seaborn`, and `scikit-learn`, must be installed separately and are widely used in data science and machine learning. The extensive availability of libraries in Python's ecosystem makes it a versatile and powerful programming language for various domains. <br>\n",
    "\n",
    "`beautifulsoup4` is a Python library used for parsing HTML and XML documents. It provides Pythonic methods for navigating, searching, and modifying the parse tree, making it ideal for web scraping tasks. BeautifulSoup is particularly useful for extracting data from web pages with inconsistent or poorly structured HTML. It works well with parsers like `html5lib` and `lxml`. <br>\n",
    "https://www.crummy.com/software/BeautifulSoup/bs4/doc/ <br>\n",
    "\n",
    "`Counter` is a subclass of Python’s `collections` module used for counting hashable objects. It allows for the quick and easy creation of frequency distributions, such as word or token counts in NLP tasks. <br>\n",
    "https://docs.python.org/3/library/collections.html#collections.Counter <br>\n",
    "\n",
    "`html5lib` is a pure-Python HTML parser designed to parse documents the same way modern web browsers do. It is especially useful for handling malformed or messy HTML. When used with `beautifulsoup4`, it provides robust parsing capabilities that help ensure accurate and tolerant extraction of web content. <br>\n",
    "https://html5lib.readthedocs.io/en/latest/ <br>\n",
    "\n",
    "`ipykernel` allows Jupyter Notebooks to run Python code by providing the kernel interface used to execute cells and handle communication between the front-end and the Python interpreter. <br>\n",
    "https://ipykernel.readthedocs.io/en/latest/ <br>\n",
    "\n",
    "`jupyterlab` is the next-generation user interface for Project Jupyter. It offers a flexible, extensible environment for interactive computing with support for code, markdown, visualizations, and terminals all within a tabbed workspace. JupyterLab enhances productivity by allowing users to organize notebooks, text editors, and data file viewers side by side. <br>\n",
    "https://jupyterlab.readthedocs.io/en/stable/ <br>\n",
    "\n",
    "`list` is a built-in Python data structure used to store ordered, mutable sequences of elements. Lists are highly versatile and are used extensively in Python programming for tasks ranging from data storage to iteration and transformation.\n",
    "https://docs.python.org/3/tutorial/datastructures.html#more-on-lists\n",
    "\n",
    "`Matplotlib` is a widely used data visualization library that allows users to create static, animated, and interactive plots. It provides extensive tools for generating various chart types, including line plots, scatter plots, histograms, and bar charts, making it a critical library for exploratory data analysis. <br>\n",
    "https://matplotlib.org/stable/contents.html <br>\n",
    "\n",
    "`notebook` is the Python package that powers the classic Jupyter Notebook interface. It provides a web-based environment for writing and running code in interactive cells, supporting rich media, visualizations, and markdown documentation. The notebook server manages the execution of kernels and renders notebooks in a browser. This tool is foundational for data analysis, teaching, and exploratory programming workflows. <br>\n",
    "https://jupyter-notebook.readthedocs.io/en/stable/ <br>\n",
    "\n",
    "`numpy` is a fundamental package for scientific computing in Python. It provides efficient support for numerical operations on large, multi-dimensional arrays and matrices, along with a wide range of mathematical functions such as mean, median, and standard deviation. It is widely used in data analysis, machine learning, and engineering.\n",
    "https://numpy.org/\n",
    "\n",
    "`Pandas` is a powerful data manipulation and analysis library that provides flexible data structures, such as DataFrames and Series. It is widely used for handling structured datasets, enabling easy data cleaning, transformation, and aggregation. Pandas is essential for data preprocessing in machine learning and statistical analysis. <br>\n",
    "https://pandas.pydata.org/docs/ <br>\n",
    "\n",
    "The `requests` library simplifies making HTTP requests in Python, allowing you to send GET, POST, and other types of requests to interact with APIs or web services. <br>\n",
    " https://docs.python-requests.org/en/latest/ <br>\n",
    "\n",
    "`spaCy` is an advanced NLP library for Python that provides tools for tokenization, part-of-speech tagging, named entity recognition, and more, using pre-trained pipelines. <br>\n",
    "https://spacy.io/ <br>\n",
    "\n",
    "`spacytextblob` is a plugin for spaCy that adds sentiment analysis capabilities by integrating TextBlob's polarity and subjectivity scores into spaCy’s pipeline. <br>\n",
    "https://github.com/AndrewIbrahim/spacy-textblob <br>\n",
    "\n",
    "`TextBlob` is a Python library for processing textual data, built on top of `nltk` and `pattern`. It provides a simple API for common natural language processing (NLP) tasks such as part-of-speech tagging, noun phrase extraction, translation, and sentiment analysis. Its intuitive design and built-in sentiment scoring functions make it especially useful for quick prototyping and educational applications. <br>\n",
    "https://textblob.readthedocs.io/en/dev/ <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77c12650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and initial setup for NLP Article Comparison Notebook\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import requests\n",
    "import pickle\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "from typing import List\n",
    "\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Load or download spaCy language model\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "except OSError:\n",
    "    import subprocess\n",
    "    subprocess.run([\"python\", \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"])\n",
    "    nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf84a58",
   "metadata": {},
   "source": [
    "#### Section 1. Extract and Save About Us for Each Company's Website\n",
    "We are writing code that extracts the full HTML content from the webpages at https://www.traderjoes.com/home/about-us, https://www.aldi.us/en/about-aldi/, https://www.albertsons.com/about-us.html, https://www.hy-vee.com/corporate/our-company/our-history/, and https://www.pigglywiggly.com/history/, then serializes the output to `.pkl` (or another appropriate format) for persistent storage. These files are saved in a directory called `dump_folder/` so they can be reliably loaded later for NLP processing and analysis. As part of the code, if we can not access the webpage, we will receive an error message, informing us of a failure to capture from the internet. <br> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31378867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Fetching Tradejoes - https://www.traderjoes.com/home/about-us\n",
      "Failed to fetch Tradejoes (https://www.traderjoes.com/home/about-us): 403 Client Error: Forbidden for url: https://www.traderjoes.com/home/about-us\n",
      "\n",
      " Fetching Aldi - https://www.aldi.us/en/about-aldi/\n",
      "HTML from https://www.aldi.us/en/about-aldi/ saved to dump_folder/aldi.pkl\n",
      "\n",
      " Fetching Albertsons - https://www.albertsons.com/about-us.html\n",
      "HTML from https://www.albertsons.com/about-us.html saved to dump_folder/albertsons.pkl\n",
      "\n",
      " Fetching Hyvee - https://www.hy-vee.com/corporate/our-company/our-history/\n",
      "HTML from https://www.hy-vee.com/corporate/our-company/our-history/ saved to dump_folder/hyvee.pkl\n",
      "\n",
      " Fetching Pigglywiggly - https://www.pigglywiggly.com/history/\n",
      "HTML from https://www.pigglywiggly.com/history/ saved to dump_folder/pigglywiggly.pkl\n"
     ]
    }
   ],
   "source": [
    "# Section 1: Extract and Save Article HTML with Header Fix, Timeout, and Logging\n",
    "\n",
    "# Create a folder to store the dumped HTML if it doesn't exist\n",
    "os.makedirs(\"dump_folder\", exist_ok=True)\n",
    "\n",
    "# User-Agent header to mimic a real browser\n",
    "headers = {\n",
    "    \"User-Agent\": (\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "        \"Chrome/115.0.0.0 Safari/537.36\"\n",
    "    )\n",
    "}\n",
    "\n",
    "# URLs to extract (you can comment out REI if it keeps stalling)\n",
    "urls = {\n",
    "    \"tradejoes\": \"https://www.traderjoes.com/home/about-us\",\n",
    "    \"aldi\": \"https://www.aldi.us/en/about-aldi/\",\n",
    "    \"albertsons\": \"https://www.albertsons.com/about-us.html\",\n",
    "    \"hyvee\": \"https://www.hy-vee.com/corporate/our-company/our-history/\",\n",
    "    \"pigglywiggly\": \"https://www.pigglywiggly.com/history/\"\n",
    "}\n",
    "\n",
    "# Loop through and extract HTML content\n",
    "for name, url in urls.items():\n",
    "    try:\n",
    "        print(f\"\\n Fetching {name.title()} - {url}\")\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        html_content = soup.prettify()\n",
    "\n",
    "        # Save to pickle file\n",
    "        with open(f\"dump_folder/{name}.pkl\", \"wb\") as f:\n",
    "            pickle.dump(html_content, f)\n",
    "\n",
    "        print(f\"HTML from {url} saved to dump_folder/{name}.pkl\")\n",
    "\n",
    "    except requests.exceptions.Timeout:\n",
    "        print(f\"Timeout: Skipping {name.title()} due to slow response.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to fetch {name.title()} ({url}): {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d4a267",
   "metadata": {},
   "source": [
    "#### Section 2. Load and Display About Us Text Conversion of Each Company's Website\n",
    "We are writing code that reads the serialized HTML files from the `dump_folder/` created in Section 1. Using BeautifulSoup, we extract the plain text content from each company's \"About Us\" page with `.get_text()` and display a preview of the text. The full parsed text is then saved as a `.txt` file into a folder named `parse/` for later NLP analysis. <br>\n",
    "\n",
    "#### Preload Filter Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a466e56d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File not found for tradejoes. Did the HTML fail to download in Section 1?\n",
      "\n",
      " Parsed Text Preview for Aldi:\n",
      "\n",
      "About ALDI  | ALDI US Help Skip to Content Products Weekly Specials Featured Recipes Search Search Search Results Log in Cart Your Cart (0 products) Your cart is empty. But full of possibilities Let's go shopping Buy Again Restock your favorites! Subtotal ‎ Continue to Checkout Products Fresh Produce Fresh Fruit Fresh Vegetables Salad Kits Flowers and Plants Healthy Living Healthy Food & Snacks Fresh Meat & Seafood Fresh Beef Fresh Poultry Fresh Pork Fresh Sausage Fresh Seafood Other Meat & Plant-Based Proteins Snacks Chips, Crackers & Popcorn Cookies & Sweets Nuts & Dried Fruit Trail Mix Jerky & Dried Meat BBQ & Picnic Grilled Meat & Seafood Cookout Essentials BBQ & Picnic Supplies Frozen Foods Frozen Meat, Poultry & Seafood Frozen Vegetables Frozen Ice Cream & Desserts Frozen Fruit Frozen Breakfast Frozen Meals & Sides Frozen Appetizers & Snacks Frozen Pizza Frozen Sandwiches Dairy & Eggs Milk & Milk Substitutes Eggs Cheese Butter Yogurt & Sour Cream Coffee Creamer Beverages Water Co...\n",
      "\n",
      "✔ Plain text saved to parse/aldi.txt\n",
      "\n",
      " Parsed Text Preview for Albertsons:\n",
      "\n",
      "About Us | Albertsons About Us Unsupported browser You're currently using an unsupported browser, which may impact the sites display and prevent key functionality. Update your browser now for greater security, speed, and the best experience on this site. View supported browsers . ✕ Skip to search Skip to main\n",
      "        content Skip to cookie settings Skip to chat Grocery Health Pharmacy For Business Welcome back! You're currently shopping with your default address: Got it Welcome back! You're currently shopping with your default address: Got it Sign in Welcome Sign In Create Account My account Buy It Again Purchases Manage Schedule & Save Profile & preferences Family members Points & rewards Wallet FreshPass My List Contact Us Sign Out Close Albertsons for U Weekly Ad Meal Plans Fresh Fresh Bread & Bakery Pitas & Flatbreads Cookies & Bakery Snacks Desserts & Cheesecakes Buns & Rolls Cakes & Cupcakes Bakery Catering Trays Breakfast & Donuts Bagels & Muffins Baguettes & French Bread Sandwi...\n",
      "\n",
      "✔ Plain text saved to parse/albertsons.txt\n",
      "\n",
      " Parsed Text Preview for Hyvee:\n",
      "\n",
      "Our History - Company - Hy-Vee - Your employee-owned grocery store Skip to main content Hy-Vee.com Contact Hy-Vee Toggle navigation Careers Careers Overview Search Hy-Vee Careers Careers Login Benefits IT Careers Hiring Heroes Interviewing Tips Employee Development and Education Our Company Our Company Overview Our History Community One Step Subsidiaries Sustainability Corporate Social Responsibility Belonging News & Events News & Events Overview Announcements New Stores News & Press Releases Promotions Contact Hy-Vee Hy-Vee.com Our History Hy-Vee was founded in the 1930's, and the rest is history. 1930 1940 1950 1960 1970 1980 1990 2000 2010 1930 With a goal to provide “good merchandise, appreciative service and low prices,” Charles Hyde (1883-1970) and David Vredenburg (1884-1949) opened a small store in Beaconsfield, Iowa , in 1930. The store was leased only a few weeks before the stock market crash of October, 1929. The two men began calling their operation Hyde & Vredenburg in 193...\n",
      "\n",
      "✔ Plain text saved to parse/hyvee.txt\n",
      "\n",
      " Parsed Text Preview for Pigglywiggly:\n",
      "\n",
      "History | Piggly Wiggly, LLC Menu Skip to content Home About Piggly Wiggly History Brands Contact Us C&S Wholesale Grocers Website Kid’s Korner Coloring Book Our Impact Community and Events News and Press Releases Store Locations Alabama Arkansas Florida Georgia Illinois Kentucky Louisiana Mississippi New York North Carolina Ohio Oklahoma South Carolina Tennessee Texas Virginia West Virginia Wisconsin Shop Logo Merchandise Careers History Brands Contact Us C&S Wholesale Grocers Website Kid’s Korner Coloring Book Our History Piggly Wiggly®, America’s first true self-service grocery store, was founded in Memphis, Tennessee in 1916 by Clarence Saunders. In grocery stores of that time, shoppers presented their orders to clerks who then gathered the goods from the store shelves. Saunders, a dynamic and innovative man, noticed that this method resulted in wasted time and expense, so he came up with an unheard-of solution that would revolutionize the entire grocery industry: he developed a wa...\n",
      "\n",
      "✔ Plain text saved to parse/pigglywiggly.txt\n",
      "✔ Loaded 14 custom exclusion terms from filters/bad_words.txt\n"
     ]
    }
   ],
   "source": [
    "# Section 2: Load and Display Article Text\n",
    "\n",
    "# Ensure output folder exists\n",
    "os.makedirs(\"parse\", exist_ok=True)\n",
    "\n",
    "# List of company identifiers matching Section 1\n",
    "companies = [\"tradejoes\", \"aldi\", \"albertsons\", \"hyvee\", \"pigglywiggly\"]\n",
    "\n",
    "for company in companies:\n",
    "    try:\n",
    "        # Load raw HTML from the dumped pickle file\n",
    "        with open(f\"dump_folder/{company}.pkl\", \"rb\") as f:\n",
    "            html_content = pickle.load(f)\n",
    "\n",
    "        # Parse HTML with BeautifulSoup\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        plain_text = soup.get_text(separator=' ', strip=True)\n",
    "\n",
    "        # Display a preview of the parsed text\n",
    "        print(f\"\\n Parsed Text Preview for {company.capitalize()}:\\n\")\n",
    "        print(plain_text[:1000] + \"...\\n\")  # Print first 1000 characters\n",
    "\n",
    "        # Save to a .txt file in the \"parse\" directory\n",
    "        with open(f\"parse/{company}.txt\", \"w\", encoding=\"utf-8\") as out_file:\n",
    "            out_file.write(plain_text)\n",
    "\n",
    "        print(f\"✔ Plain text saved to parse/{company}.txt\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found for {company}. Did the HTML fail to download in Section 1?\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {company}: {e}\")\n",
    "        \n",
    "# Pre-Load custom exclusion list from file for both tokens and lemmas\n",
    "exclude_path = \"filters/bad_words.txt\"  \n",
    "\n",
    "if os.path.exists(exclude_path):\n",
    "    with open(exclude_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        custom_exclude = set(line.strip().lower() for line in f if line.strip())\n",
    "    print(f\"✔ Loaded {len(custom_exclude)} custom exclusion terms from {exclude_path}\")\n",
    "else:\n",
    "    print(f\"⚠️ Exclusion file not found at {exclude_path}. Continuing with empty list.\")\n",
    "    custom_exclude = set() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54ca9e4",
   "metadata": {},
   "source": [
    "#### Section 3. Analyze Most Frequent Tokens with spaCy\n",
    "We are loading each company's parsed plain text from the `parse/` directory and analyzing it using a trained `spaCy` pipeline. We filter out tokens that are punctuation, stop words, or whitespace, and convert each token to lowercase. For each company, we extract and print the ten most frequent tokens along with their frequencies. We also save these top tokens in a new folder called `tps_report/` for later use in deliverables. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e2958a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Could not find parse/tradejoes.txt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h3> Top 10 Tokens per Company</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tradejoes</th>\n",
       "      <th>aldi</th>\n",
       "      <th>albertsons</th>\n",
       "      <th>hyvee</th>\n",
       "      <th>pigglywiggly</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Rank 1</th>\n",
       "      <td>N/A</td>\n",
       "      <td>aldi (28)</td>\n",
       "      <td>albertsons (19)</td>\n",
       "      <td>hy (107)</td>\n",
       "      <td>piggly (20)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rank 2</th>\n",
       "      <td>N/A</td>\n",
       "      <td>new (15)</td>\n",
       "      <td>care (17)</td>\n",
       "      <td>vee (105)</td>\n",
       "      <td>wiggly (20)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rank 3</th>\n",
       "      <td>N/A</td>\n",
       "      <td>products (13)</td>\n",
       "      <td>try (15)</td>\n",
       "      <td>store (62)</td>\n",
       "      <td>® (18)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rank 4</th>\n",
       "      <td>N/A</td>\n",
       "      <td>opens (13)</td>\n",
       "      <td>deli (14)</td>\n",
       "      <td>stores (38)</td>\n",
       "      <td>saunders (8)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rank 5</th>\n",
       "      <td>N/A</td>\n",
       "      <td>tab (13)</td>\n",
       "      <td>load (14)</td>\n",
       "      <td>company (37)</td>\n",
       "      <td>store (7)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rank 6</th>\n",
       "      <td>N/A</td>\n",
       "      <td>fresh (11)</td>\n",
       "      <td>frozen (10)</td>\n",
       "      <td>iowa (35)</td>\n",
       "      <td>grocery (7)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rank 7</th>\n",
       "      <td>N/A</td>\n",
       "      <td>frozen (11)</td>\n",
       "      <td>® (9)</td>\n",
       "      <td>new (23)</td>\n",
       "      <td>c&amp;s (6)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rank 8</th>\n",
       "      <td>N/A</td>\n",
       "      <td>cart (10)</td>\n",
       "      <td>foods (8)</td>\n",
       "      <td>opened (23)</td>\n",
       "      <td>llc (5)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rank 9</th>\n",
       "      <td>N/A</td>\n",
       "      <td>supplies (9)</td>\n",
       "      <td>sorry (8)</td>\n",
       "      <td>year (18)</td>\n",
       "      <td>history (4)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rank 10</th>\n",
       "      <td>N/A</td>\n",
       "      <td>food (8)</td>\n",
       "      <td>canned (8)</td>\n",
       "      <td>vredenburg (17)</td>\n",
       "      <td>wholesale (4)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        tradejoes           aldi       albertsons            hyvee  \\\n",
       "Rank 1        N/A      aldi (28)  albertsons (19)         hy (107)   \n",
       "Rank 2        N/A       new (15)        care (17)        vee (105)   \n",
       "Rank 3        N/A  products (13)         try (15)       store (62)   \n",
       "Rank 4        N/A     opens (13)        deli (14)      stores (38)   \n",
       "Rank 5        N/A       tab (13)        load (14)     company (37)   \n",
       "Rank 6        N/A     fresh (11)      frozen (10)        iowa (35)   \n",
       "Rank 7        N/A    frozen (11)            ® (9)         new (23)   \n",
       "Rank 8        N/A      cart (10)        foods (8)      opened (23)   \n",
       "Rank 9        N/A   supplies (9)        sorry (8)        year (18)   \n",
       "Rank 10       N/A       food (8)       canned (8)  vredenburg (17)   \n",
       "\n",
       "          pigglywiggly  \n",
       "Rank 1     piggly (20)  \n",
       "Rank 2     wiggly (20)  \n",
       "Rank 3          ® (18)  \n",
       "Rank 4    saunders (8)  \n",
       "Rank 5       store (7)  \n",
       "Rank 6     grocery (7)  \n",
       "Rank 7         c&s (6)  \n",
       "Rank 8         llc (5)  \n",
       "Rank 9     history (4)  \n",
       "Rank 10  wholesale (4)  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Saved top tokens to tps_report/tradejoes_tokens.txt\n",
      "✔ Saved top tokens to tps_report/aldi_tokens.txt\n",
      "✔ Saved top tokens to tps_report/albertsons_tokens.txt\n",
      "✔ Saved top tokens to tps_report/hyvee_tokens.txt\n",
      "✔ Saved top tokens to tps_report/pigglywiggly_tokens.txt\n"
     ]
    }
   ],
   "source": [
    "# Section 3: Analyze Most Frequent Tokens with spaCy (Tabular Output)\n",
    "\n",
    "os.makedirs(\"tps_report\", exist_ok=True)\n",
    "\n",
    "companies = [\"tradejoes\", \"aldi\", \"albertsons\", \"hyvee\", \"pigglywiggly\"]\n",
    "\n",
    "# Dictionary to store top tokens for table display\n",
    "token_table = {}\n",
    "\n",
    "for company in companies:\n",
    "    try:\n",
    "        # Load parsed text\n",
    "        with open(f\"parse/{company}.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "            text = f.read()\n",
    "\n",
    "        # Process with spaCy\n",
    "        doc = nlp(text)\n",
    "\n",
    "        # Filter and lowercase tokens\n",
    "        filtered_tokens = [\n",
    "            token.text.lower()\n",
    "            for token in doc\n",
    "            if not token.is_stop and not token.is_punct and not token.is_space\n",
    "        ]\n",
    "\n",
    "        # Count frequency and get top 10\n",
    "        token_freq = Counter(filtered_tokens)\n",
    "        top_tokens = token_freq.most_common(10)\n",
    "\n",
    "        # Save to file\n",
    "        with open(f\"tps_report/{company}_tokens.txt\", \"w\", encoding=\"utf-8\") as out_file:\n",
    "            for token, freq in top_tokens:\n",
    "                out_file.write(f\"{token}: {freq}\\n\")\n",
    "\n",
    "        # Store formatted tokens for DataFrame\n",
    "        token_table[company] = [f\"{token} ({freq})\" for token, freq in top_tokens]\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\" Could not find parse/{company}.txt\")\n",
    "        token_table[company] = [\"N/A\"] * 10\n",
    "    except Exception as e:\n",
    "        print(f\" Error processing {company}: {e}\")\n",
    "        token_table[company] = [\"Error\"] * 10\n",
    "\n",
    "# Convert to DataFrame for tabular display\n",
    "token_df = pd.DataFrame(token_table, index=[f\"Rank {i+1}\" for i in range(10)])\n",
    "\n",
    "# Display the token table\n",
    "display(HTML(\"<h3> Top 10 Tokens per Company</h3>\"))\n",
    "display(token_df)\n",
    "\n",
    "# Print saved confirmation\n",
    "for company in companies:\n",
    "    print(f\"✔ Saved top tokens to tps_report/{company}_tokens.txt\")\n",
    "    \n",
    "# Save token frequency table to CSV\n",
    "token_df.to_csv(\"tps_report/token_frequency_table.csv\", index=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b9bd78",
   "metadata": {},
   "source": [
    "#### Section 3.1. Analyze Most Frequent Tokens with spaCy with Filter\n",
    "In this section, we extend our original token frequency analysis by applying an additional layer of filtering using a custom exclusion list. This list is stored in a text file located in the `filters/` directory and contains manually specified words that are either too generic or irrelevant for our domain-specific analysis (e.g., \"company\", \"store\", \"product\"). <br>\n",
    "\n",
    "The process mirrors Section 3, where we use a trained `spaCy` pipeline to tokenize the plain text content from each company's parsed \"About Us\" page. We apply filters to remove punctuation, stop words, whitespace, and now, any word present in our exclusion list. The result is a more targeted and refined set of tokens that better represent meaningful themes and terminology used by each company. <br>\n",
    "\n",
    "We extract the ten most frequent filtered tokens per company and display them in a comparative table. Additionally, the top tokens for each company are saved to individual files in the `tps_report/` directory using the `_tokens_filtered.txt` suffix, and the full table is saved as `token_frequency_table_filtered.csv` for future reference or analysis. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cbfff816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " parse/tradejoes.txt not found.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h3> Top 10 Filtered Tokens per Company</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tradejoes</th>\n",
       "      <th>aldi</th>\n",
       "      <th>albertsons</th>\n",
       "      <th>hyvee</th>\n",
       "      <th>pigglywiggly</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Rank 1</th>\n",
       "      <td>N/A</td>\n",
       "      <td>new (15)</td>\n",
       "      <td>care (17)</td>\n",
       "      <td>iowa (35)</td>\n",
       "      <td>saunders (8)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rank 2</th>\n",
       "      <td>N/A</td>\n",
       "      <td>products (13)</td>\n",
       "      <td>try (15)</td>\n",
       "      <td>new (23)</td>\n",
       "      <td>c&amp;s (6)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rank 3</th>\n",
       "      <td>N/A</td>\n",
       "      <td>opens (13)</td>\n",
       "      <td>deli (14)</td>\n",
       "      <td>opened (23)</td>\n",
       "      <td>history (4)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rank 4</th>\n",
       "      <td>N/A</td>\n",
       "      <td>tab (13)</td>\n",
       "      <td>load (14)</td>\n",
       "      <td>year (18)</td>\n",
       "      <td>wholesale (4)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rank 5</th>\n",
       "      <td>N/A</td>\n",
       "      <td>fresh (11)</td>\n",
       "      <td>frozen (10)</td>\n",
       "      <td>vredenburg (17)</td>\n",
       "      <td>grocers (4)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rank 6</th>\n",
       "      <td>N/A</td>\n",
       "      <td>frozen (11)</td>\n",
       "      <td>sorry (8)</td>\n",
       "      <td>customers (17)</td>\n",
       "      <td>brands (3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rank 7</th>\n",
       "      <td>N/A</td>\n",
       "      <td>cart (10)</td>\n",
       "      <td>canned (8)</td>\n",
       "      <td>introduced (16)</td>\n",
       "      <td>contact (3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rank 8</th>\n",
       "      <td>N/A</td>\n",
       "      <td>supplies (9)</td>\n",
       "      <td>sign (7)</td>\n",
       "      <td>employees (13)</td>\n",
       "      <td>korner (3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rank 9</th>\n",
       "      <td>N/A</td>\n",
       "      <td>snacks (8)</td>\n",
       "      <td>unable (7)</td>\n",
       "      <td>hyde (11)</td>\n",
       "      <td>coloring (3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rank 10</th>\n",
       "      <td>N/A</td>\n",
       "      <td>meat (8)</td>\n",
       "      <td>menu (7)</td>\n",
       "      <td>des (11)</td>\n",
       "      <td>book (3)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        tradejoes           aldi   albertsons            hyvee   pigglywiggly\n",
       "Rank 1        N/A       new (15)    care (17)        iowa (35)   saunders (8)\n",
       "Rank 2        N/A  products (13)     try (15)         new (23)        c&s (6)\n",
       "Rank 3        N/A     opens (13)    deli (14)      opened (23)    history (4)\n",
       "Rank 4        N/A       tab (13)    load (14)        year (18)  wholesale (4)\n",
       "Rank 5        N/A     fresh (11)  frozen (10)  vredenburg (17)    grocers (4)\n",
       "Rank 6        N/A    frozen (11)    sorry (8)   customers (17)     brands (3)\n",
       "Rank 7        N/A      cart (10)   canned (8)  introduced (16)    contact (3)\n",
       "Rank 8        N/A   supplies (9)     sign (7)   employees (13)     korner (3)\n",
       "Rank 9        N/A     snacks (8)   unable (7)        hyde (11)   coloring (3)\n",
       "Rank 10       N/A       meat (8)     menu (7)         des (11)       book (3)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Filtered token frequency table saved to tps_report/token_frequency_table_filtered.csv\n",
      " Saved filtered top tokens to tps_report/tradejoes_tokens_filtered.txt\n",
      " Saved filtered top tokens to tps_report/aldi_tokens_filtered.txt\n",
      " Saved filtered top tokens to tps_report/albertsons_tokens_filtered.txt\n",
      " Saved filtered top tokens to tps_report/hyvee_tokens_filtered.txt\n",
      " Saved filtered top tokens to tps_report/pigglywiggly_tokens_filtered.txt\n"
     ]
    }
   ],
   "source": [
    "# Section 3.1: Analyze Most Frequent Tokens with spaCy + Custom Exclusion Filter\n",
    "\n",
    "os.makedirs(\"tps_report\", exist_ok=True)\n",
    "\n",
    "# Load exclusion list (if not loaded earlier)\n",
    "exclude_path = \"filters/bad_words.txt\"\n",
    "if os.path.exists(exclude_path):\n",
    "    with open(exclude_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        custom_exclude = set(line.strip().lower() for line in f if line.strip())\n",
    "else:\n",
    "    print(f\" Exclusion file not found at {exclude_path}. Using empty list.\")\n",
    "    custom_exclude = set()\n",
    "\n",
    "# Reuse companies list\n",
    "token_table_filtered = {}\n",
    "top_n = 10\n",
    "\n",
    "for company in companies:\n",
    "    try:\n",
    "        with open(f\"parse/{company}.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "            text = f.read()\n",
    "\n",
    "        doc = nlp(text)\n",
    "\n",
    "        # Apply filtering with custom exclusion\n",
    "        filtered_tokens = [\n",
    "            token.text.lower()\n",
    "            for token in doc\n",
    "            if not token.is_stop\n",
    "            and not token.is_punct\n",
    "            and not token.is_space\n",
    "            and token.text.lower() not in custom_exclude\n",
    "        ]\n",
    "\n",
    "        token_freq = Counter(filtered_tokens)\n",
    "        top_tokens = token_freq.most_common(top_n)\n",
    "\n",
    "        # Save to new filtered file\n",
    "        with open(f\"tps_report/{company}_tokens_filtered.txt\", \"w\", encoding=\"utf-8\") as out_file:\n",
    "            for token, freq in top_tokens:\n",
    "                out_file.write(f\"{token}: {freq}\\n\")\n",
    "\n",
    "        token_table_filtered[company] = [f\"{token} ({freq})\" for token, freq in top_tokens]\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\" parse/{company}.txt not found.\")\n",
    "        token_table_filtered[company] = [\"N/A\"] * top_n\n",
    "    except Exception as e:\n",
    "        print(f\" Error processing {company}: {e}\")\n",
    "        token_table_filtered[company] = [\"Error\"] * top_n\n",
    "\n",
    "# Create and display table\n",
    "import pandas as pd\n",
    "token_df_filtered = pd.DataFrame(token_table_filtered, index=[f\"Rank {i+1}\" for i in range(top_n)])\n",
    "display(HTML(\"<h3> Top 10 Filtered Tokens per Company</h3>\"))\n",
    "display(token_df_filtered)\n",
    "\n",
    "# Save table\n",
    "token_df_filtered.to_csv(\"tps_report/token_frequency_table_filtered.csv\", index=True)\n",
    "print(\" Filtered token frequency table saved to tps_report/token_frequency_table_filtered.csv\")\n",
    "\n",
    "# Confirmation\n",
    "for company in companies:\n",
    "    print(f\" Saved filtered top tokens to tps_report/{company}_tokens_filtered.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e138d9a3",
   "metadata": {},
   "source": [
    "#### Section 4. Analyze Most Frequent Lemmas with spaCy\n",
    "We are analyzing the same parsed text from each company's \"About Us\" page using `spaCy`, but this time we're focusing on lemmas, the base or dictionary forms of words. After filtering out stop words, punctuation, and whitespace, we identify the Top 10 most frequent lemmas for each company. We print these results in a table and save both individual and tabular outputs to the `tps_report/` folder for later analysis. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "139234a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Could not find parse/tradejoes.txt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h3> Top 10 Lemmas per Company</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tradejoes</th>\n",
       "      <th>aldi</th>\n",
       "      <th>albertsons</th>\n",
       "      <th>hyvee</th>\n",
       "      <th>pigglywiggly</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Rank 1</th>\n",
       "      <td>N/A</td>\n",
       "      <td>aldi (28)</td>\n",
       "      <td>albertsons (18)</td>\n",
       "      <td>hy (107)</td>\n",
       "      <td>piggly (20)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rank 2</th>\n",
       "      <td>N/A</td>\n",
       "      <td>price (15)</td>\n",
       "      <td>care (17)</td>\n",
       "      <td>vee (105)</td>\n",
       "      <td>wiggly (20)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rank 3</th>\n",
       "      <td>N/A</td>\n",
       "      <td>new (15)</td>\n",
       "      <td>try (15)</td>\n",
       "      <td>store (95)</td>\n",
       "      <td>® (18)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rank 4</th>\n",
       "      <td>N/A</td>\n",
       "      <td>product (14)</td>\n",
       "      <td>deli (14)</td>\n",
       "      <td>company (37)</td>\n",
       "      <td>store (11)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rank 5</th>\n",
       "      <td>N/A</td>\n",
       "      <td>open (13)</td>\n",
       "      <td>load (14)</td>\n",
       "      <td>iowa (35)</td>\n",
       "      <td>grocery (7)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rank 6</th>\n",
       "      <td>N/A</td>\n",
       "      <td>tab (13)</td>\n",
       "      <td>shop (12)</td>\n",
       "      <td>year (24)</td>\n",
       "      <td>c&amp;s (6)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rank 7</th>\n",
       "      <td>N/A</td>\n",
       "      <td>fresh (11)</td>\n",
       "      <td>frozen (10)</td>\n",
       "      <td>new (23)</td>\n",
       "      <td>llc (5)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rank 8</th>\n",
       "      <td>N/A</td>\n",
       "      <td>frozen (11)</td>\n",
       "      <td>® (9)</td>\n",
       "      <td>open (23)</td>\n",
       "      <td>saunder (5)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rank 9</th>\n",
       "      <td>N/A</td>\n",
       "      <td>cart (10)</td>\n",
       "      <td>store (9)</td>\n",
       "      <td>customer (18)</td>\n",
       "      <td>history (4)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rank 10</th>\n",
       "      <td>N/A</td>\n",
       "      <td>shop (9)</td>\n",
       "      <td>foods (8)</td>\n",
       "      <td>employee (17)</td>\n",
       "      <td>wholesale (4)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        tradejoes          aldi       albertsons          hyvee   pigglywiggly\n",
       "Rank 1        N/A     aldi (28)  albertsons (18)       hy (107)    piggly (20)\n",
       "Rank 2        N/A    price (15)        care (17)      vee (105)    wiggly (20)\n",
       "Rank 3        N/A      new (15)         try (15)     store (95)         ® (18)\n",
       "Rank 4        N/A  product (14)        deli (14)   company (37)     store (11)\n",
       "Rank 5        N/A     open (13)        load (14)      iowa (35)    grocery (7)\n",
       "Rank 6        N/A      tab (13)        shop (12)      year (24)        c&s (6)\n",
       "Rank 7        N/A    fresh (11)      frozen (10)       new (23)        llc (5)\n",
       "Rank 8        N/A   frozen (11)            ® (9)      open (23)    saunder (5)\n",
       "Rank 9        N/A     cart (10)        store (9)  customer (18)    history (4)\n",
       "Rank 10       N/A      shop (9)        foods (8)  employee (17)  wholesale (4)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Lemma frequency table saved to tps_report/lemma_frequency_table.csv\n",
      "✔ Saved top lemmas to tps_report/tradejoes_lemmas.txt\n",
      "✔ Saved top lemmas to tps_report/aldi_lemmas.txt\n",
      "✔ Saved top lemmas to tps_report/albertsons_lemmas.txt\n",
      "✔ Saved top lemmas to tps_report/hyvee_lemmas.txt\n",
      "✔ Saved top lemmas to tps_report/pigglywiggly_lemmas.txt\n"
     ]
    }
   ],
   "source": [
    "# Section 4: Analyze Most Frequent Lemmas with spaCy\n",
    "\n",
    "lemma_table = {}\n",
    "top_n = 10  # Top 10 lemmas\n",
    "\n",
    "for company in companies:\n",
    "    try:\n",
    "        # Load parsed text\n",
    "        with open(f\"parse/{company}.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "            text = f.read()\n",
    "\n",
    "        # Process with spaCy\n",
    "        doc = nlp(text)\n",
    "\n",
    "        # Filter and extract lemmas\n",
    "        filtered_lemmas = [\n",
    "            token.lemma_.lower()\n",
    "            for token in doc\n",
    "            if not token.is_stop and not token.is_punct and not token.is_space\n",
    "        ]\n",
    "\n",
    "        # Count and get top N lemmas\n",
    "        lemma_freq = Counter(filtered_lemmas)\n",
    "        top_lemmas = lemma_freq.most_common(top_n)\n",
    "\n",
    "        # Save to file\n",
    "        with open(f\"tps_report/{company}_lemmas.txt\", \"w\", encoding=\"utf-8\") as out_file:\n",
    "            for lemma, freq in top_lemmas:\n",
    "                out_file.write(f\"{lemma}: {freq}\\n\")\n",
    "\n",
    "        # Store for table\n",
    "        lemma_table[company] = [f\"{lemma} ({freq})\" for lemma, freq in top_lemmas]\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\" Could not find parse/{company}.txt\")\n",
    "        lemma_table[company] = [\"N/A\"] * top_n\n",
    "    except Exception as e:\n",
    "        print(f\" Error processing {company}: {e}\")\n",
    "        lemma_table[company] = [\"Error\"] * top_n\n",
    "\n",
    "# Create and display lemma frequency table\n",
    "lemma_df = pd.DataFrame(lemma_table, index=[f\"Rank {i+1}\" for i in range(top_n)])\n",
    "\n",
    "display(HTML(\"<h3> Top 10 Lemmas per Company</h3>\"))\n",
    "display(lemma_df)\n",
    "\n",
    "# Save to CSV\n",
    "lemma_df.to_csv(\"tps_report/lemma_frequency_table.csv\", index=True)\n",
    "print(\" Lemma frequency table saved to tps_report/lemma_frequency_table.csv\")\n",
    "\n",
    "# Confirmation messages\n",
    "for company in companies:\n",
    "    print(f\"✔ Saved top lemmas to tps_report/{company}_lemmas.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f488b5",
   "metadata": {},
   "source": [
    "#### Section 4.1. Analyze Most Frequent Lemmas with spaCy with Filter\n",
    "In this section, we extend our lemma frequency analysis by applying the custom exclusion list loaded earlier from the `filters/` directory. This list contains terms we want to omit from our results due to their generic or domain-irrelevant nature. <br>\n",
    "\n",
    "Using a trained `spaCy` pipeline, we extract and lemmatize tokens from each company's parsed text. We remove stop words, punctuation, whitespace, and any lemma present in the custom exclusion list. We then identify the ten most frequent filtered lemmas for each company and present them in a table for easy comparison. <br>\n",
    "\n",
    "Each company's top lemmas are also saved individually in the `tps_report/` directory using a `_lemmas_filtered.txt` suffix, and the complete table is saved as `lemma_frequency_table_filtered.csv` for future analysis. <br.>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "58514127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " parse/tradejoes.txt not found.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h3> Top 10 Filtered Lemmas per Company</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tradejoes</th>\n",
       "      <th>aldi</th>\n",
       "      <th>albertsons</th>\n",
       "      <th>hyvee</th>\n",
       "      <th>pigglywiggly</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Rank 1</th>\n",
       "      <td>N/A</td>\n",
       "      <td>price (15)</td>\n",
       "      <td>care (17)</td>\n",
       "      <td>iowa (35)</td>\n",
       "      <td>c&amp;s (6)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rank 2</th>\n",
       "      <td>N/A</td>\n",
       "      <td>new (15)</td>\n",
       "      <td>try (15)</td>\n",
       "      <td>year (24)</td>\n",
       "      <td>saunder (5)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rank 3</th>\n",
       "      <td>N/A</td>\n",
       "      <td>product (14)</td>\n",
       "      <td>deli (14)</td>\n",
       "      <td>new (23)</td>\n",
       "      <td>history (4)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rank 4</th>\n",
       "      <td>N/A</td>\n",
       "      <td>open (13)</td>\n",
       "      <td>load (14)</td>\n",
       "      <td>open (23)</td>\n",
       "      <td>wholesale (4)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rank 5</th>\n",
       "      <td>N/A</td>\n",
       "      <td>tab (13)</td>\n",
       "      <td>shop (12)</td>\n",
       "      <td>customer (18)</td>\n",
       "      <td>grocers (4)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rank 6</th>\n",
       "      <td>N/A</td>\n",
       "      <td>fresh (11)</td>\n",
       "      <td>frozen (10)</td>\n",
       "      <td>employee (17)</td>\n",
       "      <td>service (4)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rank 7</th>\n",
       "      <td>N/A</td>\n",
       "      <td>frozen (11)</td>\n",
       "      <td>sorry (8)</td>\n",
       "      <td>vredenburg (17)</td>\n",
       "      <td>brands (3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rank 8</th>\n",
       "      <td>N/A</td>\n",
       "      <td>cart (10)</td>\n",
       "      <td>canned (8)</td>\n",
       "      <td>introduce (16)</td>\n",
       "      <td>contact (3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rank 9</th>\n",
       "      <td>N/A</td>\n",
       "      <td>shop (9)</td>\n",
       "      <td>sign (7)</td>\n",
       "      <td>service (11)</td>\n",
       "      <td>korner (3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rank 10</th>\n",
       "      <td>N/A</td>\n",
       "      <td>supplies (9)</td>\n",
       "      <td>unable (7)</td>\n",
       "      <td>hyde (11)</td>\n",
       "      <td>coloring (3)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        tradejoes          aldi   albertsons            hyvee   pigglywiggly\n",
       "Rank 1        N/A    price (15)    care (17)        iowa (35)        c&s (6)\n",
       "Rank 2        N/A      new (15)     try (15)        year (24)    saunder (5)\n",
       "Rank 3        N/A  product (14)    deli (14)         new (23)    history (4)\n",
       "Rank 4        N/A     open (13)    load (14)        open (23)  wholesale (4)\n",
       "Rank 5        N/A      tab (13)    shop (12)    customer (18)    grocers (4)\n",
       "Rank 6        N/A    fresh (11)  frozen (10)    employee (17)    service (4)\n",
       "Rank 7        N/A   frozen (11)    sorry (8)  vredenburg (17)     brands (3)\n",
       "Rank 8        N/A     cart (10)   canned (8)   introduce (16)    contact (3)\n",
       "Rank 9        N/A      shop (9)     sign (7)     service (11)     korner (3)\n",
       "Rank 10       N/A  supplies (9)   unable (7)        hyde (11)   coloring (3)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Filtered lemma frequency table saved to tps_report/lemma_frequency_table_filtered.csv\n",
      " Saved filtered top lemmas to tps_report/tradejoes_lemmas_filtered.txt\n",
      " Saved filtered top lemmas to tps_report/aldi_lemmas_filtered.txt\n",
      " Saved filtered top lemmas to tps_report/albertsons_lemmas_filtered.txt\n",
      " Saved filtered top lemmas to tps_report/hyvee_lemmas_filtered.txt\n",
      " Saved filtered top lemmas to tps_report/pigglywiggly_lemmas_filtered.txt\n"
     ]
    }
   ],
   "source": [
    "# Section 4.1: Analyze Most Frequent Lemmas with spaCy and Custom Exclusion Filter\n",
    "\n",
    "os.makedirs(\"tps_report\", exist_ok=True)\n",
    "\n",
    "# Assume custom_exclude is already loaded from earlier cell\n",
    "lemma_table_filtered = {}\n",
    "top_n = 10\n",
    "\n",
    "for company in companies:\n",
    "    try:\n",
    "        with open(f\"parse/{company}.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "            text = f.read()\n",
    "\n",
    "        doc = nlp(text)\n",
    "\n",
    "        # Filter lemmas with custom exclusions\n",
    "        filtered_lemmas = [\n",
    "            token.lemma_.lower()\n",
    "            for token in doc\n",
    "            if not token.is_stop\n",
    "            and not token.is_punct\n",
    "            and not token.is_space\n",
    "            and token.lemma_.lower() not in custom_exclude\n",
    "        ]\n",
    "\n",
    "        lemma_freq = Counter(filtered_lemmas)\n",
    "        top_lemmas = lemma_freq.most_common(top_n)\n",
    "\n",
    "        # Save individual lemma results\n",
    "        with open(f\"tps_report/{company}_lemmas_filtered.txt\", \"w\", encoding=\"utf-8\") as out_file:\n",
    "            for lemma, freq in top_lemmas:\n",
    "                out_file.write(f\"{lemma}: {freq}\\n\")\n",
    "\n",
    "        # Save to table\n",
    "        lemma_table_filtered[company] = [f\"{lemma} ({freq})\" for lemma, freq in top_lemmas]\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\" parse/{company}.txt not found.\")\n",
    "        lemma_table_filtered[company] = [\"N/A\"] * top_n\n",
    "    except Exception as e:\n",
    "        print(f\" Error processing {company}: {e}\")\n",
    "        lemma_table_filtered[company] = [\"Error\"] * top_n\n",
    "\n",
    "# Create and display table\n",
    "lemma_df_filtered = pd.DataFrame(lemma_table_filtered, index=[f\"Rank {i+1}\" for i in range(top_n)])\n",
    "display(HTML(\"<h3> Top 10 Filtered Lemmas per Company</h3>\"))\n",
    "display(lemma_df_filtered)\n",
    "\n",
    "# Save to CSV\n",
    "lemma_df_filtered.to_csv(\"tps_report/lemma_frequency_table_filtered.csv\", index=True)\n",
    "print(\" Filtered lemma frequency table saved to tps_report/lemma_frequency_table_filtered.csv\")\n",
    "\n",
    "# Confirmation\n",
    "for company in companies:\n",
    "    print(f\" Saved filtered top lemmas to tps_report/{company}_lemmas_filtered.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
